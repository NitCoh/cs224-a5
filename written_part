1a. Yes it is the same, simply because the size of the parameters in conv-nets architectures
depend only in the number of filters and their size at each layer.
Each word get represented by a set of fixed amount of filters/kernels applied to each window on 1 dimension
and those filters are directly corresponds to weights and as a result the amount of filters determines the size
of the final embedding.


1b. There must exist at least one window for all possible values of m_word.
So, in case we assume that min(m_word) = 1. (no empty words)
the size of the kernel is 5 and the the size of the word 
is min(m_word)+2+(2*pad)= 1 + 2 + (2*pad), because the lowest value of m_word 
determines the minimal required padding.
So we want to fit 1 window with size 5,
we need pad=1 so the we can apply at least 1 time the kernel to the word.

1c. When projecting x_convout to the same vector space,
we aim learn a linear operator that represent our x_convout better in terms
of which of the applied max pooled kernels values (x_convout[i]) are important in our representation.
When varying smoothly across the identity operator and some other operators, the net can
"decide" either to leave the values (because the current representation is good) untouched
or tweak some them to improve the embedding.

because x_convout got ReLU one step before, all it's values are >= 0,
so I think the bias should be negative, to reduce the total bias in the values
passed to the sigmoid after the projection.

1d. First is the parallel option, in our NMT model in order to get representation
of the whole sequence to feed the decoder, we had to sequentially use the last hidden state 
in each time step, in contrast to Transformers which you can compute the whole sequence representation
in parallel for each word and then feed it to the next layer.

Second is the self-attention and multi-head attention module, 
in BiLSTM encoder we also try to get "attention" to our context by feeding the next
time step in the last hidden state which controlled by gates, but in practice
if the sequence is long, the information of far word in our context can literally 
be "forgotten" over time steps, in contrast to Transformer where we directly use all of
 context using the self-attention mechanism. (weight sum which determines which of the words
 in the context are important while encoding).
 
 The third is the multi-head attention which helps the net to track many "patterns"
 while encoding, just like gaining many activation maps in CNNs and learn a lot of filters in an image
 where each of them is responsible to detect it's own pattern.
 
 
 
###Analysing NMT:

##a
traduces, traduzca, tranduzcas are the words that don't occur in our spanish vocab.
The problem is that when we encode/decode with our word-based NMT system,
those words will appear as <unk> encoding/decoding, even though we have the "base" word in our vocab.
Our character-aware NMT will encode the word through it's characters which all appear in the vocab,
therefore we won't use the <unk> embeddings and also while decoding, when we encounter any OOV word,
we will apply the char-decoder which will may generate different forms of the "base" words, though
won't output <unk>.
 
##b

word - close neighbour:
Word2Vec:
- financial - economic
- neuron - nerve
- Francisco - san
- naturally - occurring
- expectation - norms

character-aware NMT embeddings:
- financial - vertical
- neuron - Newton
- Francisco - France
- naturally - practically
- expectation - exception

The similarity modeled by Word2Vec is mostly the context similarity, which means
if word A mostly seen context is the context mostly seen by B, they are probably similar.
In contrast to character-aware NMT, which model the morphological similarity between words,
e.g naturally - practically, there is no close-meaning similarity between them, but
morphologically they are build almost the same.

##c
src: Puedo vestirme como agricultor, o con ropa de cuero, y nunca nadie ha elegido un agricultor.
ref: You can have me as a farmer, or in leathers,  and no one has ever chose farmer.
a4NMT: I can <unk> like farmer, or with leather <unk> and no one has chosen a farmer.
a5NMT: I can dress as a farmer, or a leather clothing, and never ever chose a farmer.


src: Mi crculo comenz en los aos '60 en la escuela media, en Stow, Ohio donde yo era el raro de la clase.
ref: My circle began back in the '60s  in high school in Stow, Ohio  where I was the class queer.
a4NMT: My circle started in the year <unk> at the middle school, in Ohio -- where I was the rare of the class.
a5NMT: My circle started in the 1960s in the middle school, in State, Ohio, where I was the rare of class.

